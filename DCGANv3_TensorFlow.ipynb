{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGANv3_TensorFlow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "uHzYDuEqqxWN"
      },
      "cell_type": "markdown",
      "source": [
        "#DCGAN in Tensorflow\n",
        "\n",
    
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "vC5n2BLOCi_L",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%matplotlib inline\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "sqylBK0uqxFw",
        "outputId": "1c39f683-5cff-4bdb-b00d-847773ffa50f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        }
      },
      "cell_type": "code",
      "source": [
        "#Import required libraries\n",
        "%autoreload 2\n",
        "!pip install --upgrade pip\n",
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install tensorboardx\n",
        "!pip install pillow\n",
        "!pip install dicom\n",
        "!pip install opencv-python\n",
        "\n",
        "import os\n",
        "import errno\n",
        "import scipy\n",
        "import dicom\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.slim as slim\n",
        "from tensorflow import nn, layers\n",
        "from tensorflow.contrib import layers as clayers \n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision.utils as vutils\n",
        "from torchvision import transforms, utils, datasets\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "from IPython import display\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from getpass import getpass\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pip in /usr/local/lib/python3.6/dist-packages (18.1)\n",
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/60/66415660aa46b23b5e1b72bc762e816736ce8d7260213e22365af51e8f9c/torch-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (591.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 591.8MB 29kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x6271a000 @  0x7f46f7d862a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-1.0.0\n",
            "Collecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 12.3MB/s \n",
            "\u001b[?25hInstalling collected packages: pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torchvision-0.2.1\n",
            "Collecting tensorboardx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/10/0cc87d34b4a02109bee5f7b9bd9c95524fbb540311f6fbcc3758591a3f3a/tensorboardX-1.5-py2.py3-none-any.whl (112kB)\n",
            "\u001b[K    100% |████████████████████████████████| 112kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardx) (3.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardx) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardx) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardx) (40.6.3)\n",
            "Installing collected packages: tensorboardx\n",
            "Successfully installed tensorboardx-1.5\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (5.3.0)\n",
            "Collecting dicom\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/0e/488becc361f1416f6682a9b9c223eb6a9db2d5d093fb46ea91c762eb6e4a/dicom-0.9.9.post1-py3-none-any.whl (766kB)\n",
            "\u001b[K    100% |████████████████████████████████| 768kB 21.6MB/s \n",
            "\u001b[?25hInstalling collected packages: dicom\n",
            "Successfully installed dicom-0.9.9.post1\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (3.4.4.19)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.14.6)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/dicom/__init__.py:53: UserWarning: \n",
            "This code is using an older version of pydicom, which is no longer \n",
            "maintained as of Jan 2017.  You can access the new pydicom features and API \n",
            "by installing `pydicom` from PyPI.\n",
            "See 'Transitioning to pydicom 1.x' section at pydicom.readthedocs.org \n",
            "for more information.\n",
            "\n",
            "  warnings.warn(msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "G-LNd7eMf4O9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "53d501ec-2315-499d-808c-724341506062"
      },
      "cell_type": "code",
      "source": [
        "class Logger:\n",
        "\n",
        "    def __init__(self, model_name, data_name):\n",
        "        self.model_name = model_name\n",
        "        self.data_name = data_name\n",
        "\n",
        "        self.comment = '{}_{}'.format(model_name, data_name)\n",
        "        self.data_subdir = '{}/{}'.format(model_name, data_name)\n",
        "\n",
        "        # TensorBoard\n",
        "        self.writer = SummaryWriter(comment=self.comment)\n",
        "\n",
        "    def log(self, d_error, g_error, epoch, n_batch, num_batches):\n",
        "\n",
        "        # var_class = torch.autograd.variable.Variable\n",
        "        if isinstance(d_error, torch.autograd.Variable):\n",
        "            d_error = d_error.data.cpu().numpy()\n",
        "        if isinstance(g_error, torch.autograd.Variable):\n",
        "            g_error = g_error.data.cpu().numpy()\n",
        "\n",
        "        step = Logger._step(epoch, n_batch, num_batches)\n",
        "        self.writer.add_scalar(\n",
        "            '{}/D_error'.format(self.comment), d_error, step)\n",
        "        self.writer.add_scalar(\n",
        "            '{}/G_error'.format(self.comment), g_error, step)\n",
        "\n",
        "    def log_images(self, images, num_images, epoch, n_batch, num_batches, format='NCHW', normalize=True):\n",
        "        '''\n",
        "        input images are expected in format (NCHW)\n",
        "        '''\n",
        "        if type(images) == np.ndarray:\n",
        "            images = torch.from_numpy(images)\n",
        "        \n",
        "        if format=='NHWC':\n",
        "            images = images.transpose(1,3)\n",
        "        \n",
        "\n",
        "        step = Logger._step(epoch, n_batch, num_batches)\n",
        "        img_name = '{}/images{}'.format(self.comment, '')\n",
        "\n",
        "        # Make horizontal grid from image tensor\n",
        "        horizontal_grid = vutils.make_grid(\n",
        "            images, normalize=normalize, scale_each=True)\n",
        "        # Make vertical grid from image tensor\n",
        "        nrows = int(np.sqrt(num_images))\n",
        "        grid = vutils.make_grid(\n",
        "            images, nrow=nrows, normalize=True, scale_each=True)\n",
        "\n",
        "        # Add horizontal images to tensorboard\n",
        "        self.writer.add_image(img_name, horizontal_grid, step)\n",
        "\n",
        "        # Save plots\n",
        "        self.save_torch_images(horizontal_grid, grid, epoch, n_batch)\n",
        "\n",
        "    def save_torch_images(self, horizontal_grid, grid, epoch, n_batch, plot_horizontal=True):\n",
        "        out_dir = './data/images/{}'.format(self.data_subdir)\n",
        "        Logger._make_dir(out_dir)\n",
        "\n",
        "        # Plot and save horizontal\n",
        "        fig = plt.figure(figsize=(16, 16))\n",
        "        plt.imshow(np.moveaxis(horizontal_grid.numpy(), 0, -1))\n",
        "        plt.axis('off')\n",
        "        if plot_horizontal:\n",
        "            display.display(plt.gcf())\n",
        "        self._save_images(fig, epoch, n_batch, 'hori')\n",
        "        plt.close()\n",
        "\n",
        "        # Save squared\n",
        "        fig = plt.figure()\n",
        "        plt.imshow(np.moveaxis(grid.numpy(), 0, -1))\n",
        "        plt.axis('off')\n",
        "        self._save_images(fig, epoch, n_batch)\n",
        "        plt.close()\n",
        "\n",
        "    def _save_images(self, fig, epoch, n_batch, comment=''):\n",
        "        out_dir = './data/images/{}'.format(self.data_subdir)\n",
        "        Logger._make_dir(out_dir)\n",
        "        fig.savefig('{}/{}_epoch_{}_batch_{}.png'.format(out_dir,\n",
        "                                                         comment, epoch, n_batch))\n",
        "\n",
        "    def display_status(self, epoch, num_epochs, n_batch, num_batches, d_error, g_error, d_pred_real, d_pred_fake):\n",
        "        \n",
        "        # var_class = torch.autograd.variable.Variable\n",
        "        if isinstance(d_error, torch.autograd.Variable):\n",
        "            d_error = d_error.data.cpu().numpy()\n",
        "        if isinstance(g_error, torch.autograd.Variable):\n",
        "            g_error = g_error.data.cpu().numpy()\n",
        "        if isinstance(d_pred_real, torch.autograd.Variable):\n",
        "            d_pred_real = d_pred_real.data\n",
        "        if isinstance(d_pred_fake, torch.autograd.Variable):\n",
        "            d_pred_fake = d_pred_fake.data\n",
        "        \n",
        "        \n",
        "        print('Epoch: [{}/{}], Batch Num: [{}/{}]'.format(\n",
        "            epoch,num_epochs, n_batch, num_batches)\n",
        "             )\n",
        "        print('Discriminator Loss: {:.4f}, Generator Loss: {:.4f}'.format(d_error, g_error))\n",
        "        print('D(x): {:.4f}, D(G(z)): {:.4f}'.format(d_pred_real.mean(), d_pred_fake.mean()))\n",
        "\n",
        "    def save_models(self, generator, discriminator, epoch):\n",
        "        out_dir = './data/models/{}'.format(self.data_subdir)\n",
        "        Logger._make_dir(out_dir)\n",
        "        torch.save(generator.state_dict(),\n",
        "                   '{}/G_epoch_{}'.format(out_dir, epoch))\n",
        "        torch.save(discriminator.state_dict(),\n",
        "                   '{}/D_epoch_{}'.format(out_dir, epoch))\n",
        "\n",
        "    def close(self):\n",
        "        self.writer.close()\n",
        "\n",
        "    # Private Functionality\n",
        "\n",
        "    @staticmethod\n",
        "    def _step(epoch, n_batch, num_batches):\n",
        "        return epoch * num_batches + n_batch\n",
        "\n",
        "    @staticmethod\n",
        "    def _make_dir(directory):\n",
        "        try:\n",
        "            os.makedirs(directory)\n",
        "        except OSError as e:\n",
        "            if e.errno != errno.EEXIST:\n",
        "                raise\n",
        "                "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[autoreload of PIL.Image failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ImportError: cannot import name 'py3'\n",
            "]\n",
            "[autoreload of PIL._binary failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ImportError: cannot import name 'py3'\n",
            "]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ptyzbtBuUtYp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Create OASIS Dataset Class"
      ]
    },
    {
      "metadata": {
        "id": "P7rgl2lKUs2m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class OASISDataset(Dataset):\n",
        "          \n",
        "    def __init__(self, arr_name = \"T1_DCM\", transforms=None):\n",
        "        \n",
        "        self.transforms = transforms\n",
        "        self.FLAIR_DCM = []\n",
        "        self.ROI_DCM = []\n",
        "        self.T1_DCM = []\n",
        "        self.T2_DCM = []\n",
        "        self.data = torch.zeros(()).new_empty((22, 512, 512))\n",
        "        \n",
        "        user = getpass('BitBucket user')\n",
        "        password = getpass('BitBucket password')\n",
        "        os.environ['BITBUCKET_AUTH'] = user + ':' + password.replace(\"@\", \"%40\")\n",
        "\n",
        "        !git clone https://$BITBUCKET_AUTH@bitbucket.org/sidijju/OASIS-Data.git\n",
        "        os.chdir(\"OASIS-Data\")\n",
        "        \n",
        "        root = \"/content/OASIS-Data/BRAINIX/DICOM/\"\n",
        "\n",
        "        for dirName, subdirList, fileList in os.walk(\"/content/OASIS-Data/BRAINIX/DICOM\"):\n",
        "          if(dirName == root + \"FLAIR\"):\n",
        "            for filename in fileList:\n",
        "              self.FLAIR_DCM.append(os.path.join(dirName,filename))\n",
        "          if(dirName == root + \"ROI\"):\n",
        "            for filename in fileList:\n",
        "              self.ROI_DCM.append(os.path.join(dirName,filename))\n",
        "          if(dirName == root + \"T1\"):\n",
        "            for filename in fileList:\n",
        "              self.T1_DCM.append(os.path.join(dirName,filename))\n",
        "          if(dirName == root + \"T2\"):\n",
        "            for filename in fileList:\n",
        "              self.T2_DCM.append(os.path.join(dirName,filename))\n",
        "\n",
        "        #self.FLAIR_Ref = self.getInfo(self.FLAIR_DCM)\n",
        "        #self.FLAIR_Dicom = np.zeros(self.FLAIR_Ref[1], dtype=self.FLAIR_Ref[0].pixel_array.dtype)\n",
        "        #self.storeList(self.FLAIR_DCM, self.FLAIR_Dicom)\n",
        "\n",
        "        #self.ROI_Ref = self.getInfo(self.ROI_DCM)\n",
        "        #self.ROI_Dicom = np.zeros(self.ROI_Ref[1], dtype=self.ROI_Ref[0].pixel_array.dtype)\n",
        "        #self.storeList(self.ROI_DCM, self.ROI_Dicom)\n",
        "        \n",
        "        self.T1_Ref = self.getInfo(self.T1_DCM)\n",
        "        self.T1_Dicom = np.zeros(self.T1_Ref[1], dtype=self.T1_Ref[0].pixel_array.dtype)\n",
        "        self.storeList(self.T1_DCM, self.T1_Dicom)\n",
        "\n",
        "        #self.T2_Ref = self.getInfo(self.T2_DCM)\n",
        "        #self.T2_Dicom = np.zeros(self.T2_Ref[1], dtype=self.T2_Ref[0].pixel_array.dtype)\n",
        "        #self.storeList(self.T2_DCM, self.T2_Dicom)\n",
        "        \n",
        "        if(arr_name == \"T1_DCM\"):\n",
        "          self.arr = self.T1_Dicom.reshape((512, 512, 1, 22))\n",
        "        elif(arr_name == \"T2_DCM\"):\n",
        "          self.arr = self.T2_Dicom\n",
        "        elif(arr_name == \"FLAIR_DCM\"):\n",
        "          self.arr = self.FLAIR_Dicom\n",
        "        else:\n",
        "          self.arr = self.ROI_Dicom \n",
        "        \n",
        "    def getInfo(self, ref):\n",
        "      \n",
        "          # Get ref file\n",
        "          RefDs = dicom.read_file(ref[0])\n",
        "\n",
        "          # Load dimensions based on the number of rows, columns, and slices (along the Z axis)\n",
        "          ConstPixelDims = (int(RefDs.Rows), int(RefDs.Columns), len(ref))\n",
        "          # Load spacing values (in mm)\n",
        "          ConstPixelSpacing = (float(RefDs.PixelSpacing[0]), float(RefDs.PixelSpacing[1]), float(RefDs.SliceThickness))\n",
        "          #calculate axes\n",
        "          x = np.arange(0.0, (ConstPixelDims[0]+1)*ConstPixelSpacing[0], ConstPixelSpacing[0])\n",
        "          y = np.arange(0.0, (ConstPixelDims[1]+1)*ConstPixelSpacing[1], ConstPixelSpacing[1])\n",
        "          z = np.arange(0.0, (ConstPixelDims[2]+1)*ConstPixelSpacing[2], ConstPixelSpacing[2])\n",
        "\n",
        "          return RefDs, ConstPixelDims, ConstPixelSpacing, x, y, z\n",
        "\n",
        "    def storeList(self, directory, array):\n",
        "      for filenameDCM in directory:\n",
        "        ds = dicom.read_file(filenameDCM)\n",
        "        array[:, :, int(filenameDCM[-6:-4]) - 1] = ds.pixel_array\n",
        "            \n",
        "    def plotPicture(self, im, ref, title=\"\"):\n",
        "        plt.figure(dpi=50)\n",
        "        plt.axes().set_aspect('equal', 'datalim')\n",
        "        plt.set_cmap(plt.gray())\n",
        "        plt.title(title)\n",
        "        plt.pcolormesh(self.T1_Ref[3], self.T1_Ref[4], im)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if index < len(self.arr) and index >= 0:\n",
        "          if self.transforms is not None:\n",
        "            self.plotPicture(self.arr[:, :, :, index].reshape((512, 512)), self.T1_Ref)\n",
        "            self.data.add(self.transforms(self.arr[:, :, :, index].astype('uint8')))\n",
        "          return self.arr[:, :, :, index].astype('uint8')\n",
        "        else:\n",
        "          print(\"INDEX INVALID\")\n",
        "          return None\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.arr)\n",
        "      \n",
        "      \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "0x6EWFhDvOOf"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "A67gpZaDvNJH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DATA_FOLDER = './tf_data/DCGAN/OASIS'\n",
        "def oasis_data():\n",
        "    compose = transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            #transforms.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
        "        ])\n",
        "    out_dir = '{}/dataset'.format(DATA_FOLDER)\n",
        "    return OASISDataset(arr_name = \"T1_DCM\", transforms = compose)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YiuX1WszyDvv",
        "outputId": "f8d9da1a-ae16-4661-f62f-9e2b1f1c84ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "dataset = oasis_data()\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BitBucket user··········\n",
            "BitBucket password··········\n",
            "Cloning into 'OASIS-Data'...\n",
            "remote: Counting objects: 119, done.\u001b[K\n",
            "remote: Compressing objects: 100% (86/86), done.\u001b[K\n",
            "remote: Total 119 (delta 28), reused 119 (delta 28)\u001b[K\n",
            "Receiving objects: 100% (119/119), 189.42 MiB | 28.46 MiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n",
            "Checking out files: 100% (102/102), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J9Jl7G9zhJ6x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "IMAGES_SHAPE = (512, 512)\n",
        "NOISE_SIZE = 4096"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8Mbzhwd4y1Ii",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
        "     with tf.variable_scope(name):\n",
        "        f1 = 0.5 * (1 + leak)\n",
        "        f2 = 0.5 * (1 - leak)\n",
        "        return f1 * x + f2 * abs(x)\n",
        "      \n",
        "def default_conv2d(inputs, filters):\n",
        "    return layers.conv2d(\n",
        "        inputs,\n",
        "        filters=filters,\n",
        "        kernel_size=4,\n",
        "        strides=(2, 2),\n",
        "        padding='same',\n",
        "        data_format='channels_last',\n",
        "        use_bias=False,\n",
        "    )\n",
        "\n",
        "def default_conv2d_transpose(inputs, filters):\n",
        "    return layers.conv2d_transpose(\n",
        "        inputs,\n",
        "        filters=filters,\n",
        "        kernel_size=4,\n",
        "        strides=(2, 2),\n",
        "        padding='same',\n",
        "        data_format='channels_last',\n",
        "        use_bias=False,\n",
        "        \n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "obK_ltssz32T"
      },
      "cell_type": "markdown",
      "source": [
        "## Define Networks"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "fUsXav_J0EIM"
      },
      "cell_type": "markdown",
      "source": [
        "### The Generator\n",
        "\n",
        "The generator takes a vector of random numbers and transforms it into a 512x512 image. Each layer in the network involves a strided transpose convolution, batch normalization, and rectified nonlinearity. Tensorflow's slim library allows us to easily define each of these layers."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qHZ0AWgYz3n8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generator(z):\n",
        "    with tf.variable_scope(\"generator\", reuse=tf.AUTO_REUSE):\n",
        "        zP = slim.fully_connected(z,32*32*256,normalizer_fn=slim.batch_norm,\\\n",
        "        activation_fn=tf.nn.relu,scope='g_project',weights_initializer=initializer)\n",
        "        zCon = tf.reshape(zP,[-1,32,32,256])\n",
        "    \n",
        "        gen1 = slim.convolution2d_transpose(\\\n",
        "        zCon,num_outputs=256,kernel_size=[5,5],stride=[2,2],\\\n",
        "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
        "        activation_fn=tf.nn.relu,scope='g_conv1', weights_initializer=initializer)\n",
        "    \n",
        "        gen2 = slim.convolution2d_transpose(\\\n",
        "        gen1,num_outputs=128,kernel_size=[5,5],stride=[2,2],\\\n",
        "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
        "        activation_fn=tf.nn.relu,scope='g_conv2', weights_initializer=initializer)\n",
        "    \n",
        "        gen3 = slim.convolution2d_transpose(\\\n",
        "        gen2,num_outputs=512,kernel_size=[5,5],stride=[2,2],\\\n",
        "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
        "        activation_fn=tf.nn.relu,scope='g_conv3', weights_initializer=initializer)\n",
        "    \n",
        "        g_out = slim.convolution2d_transpose(\\\n",
        "        gen3,num_outputs=1,kernel_size=[512,512],padding=\"SAME\",\\\n",
        "        biases_initializer=None,activation_fn=tf.nn.tanh,\\\n",
        "        scope='g_out', weights_initializer=initializer)\n",
        "    \n",
        "    return g_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lR4ZVkIS0IEI"
      },
      "cell_type": "markdown",
      "source": [
        "### The Discriminator"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "tri4-arg0OOz"
      },
      "cell_type": "markdown",
      "source": [
        "The discriminator network takes as input a 512x512 image and transforms it into a single valued probability of being generated from real-world data. Again we use tf.slim to define the convolutional layers, batch normalization, and weight initialization."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "o_NdkK4O0J3r",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def discriminator(bottom, reuse=False):\n",
        "    with tf.variable_scope(\"discriminator\", reuse=tf.AUTO_REUSE):\n",
        "        \n",
        "        dis1 = slim.convolution2d(bottom,512,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
        "        biases_initializer=None,activation_fn=lrelu,\\\n",
        "        reuse=reuse,scope='d_conv1',weights_initializer=initializer)\n",
        "    \n",
        "        dis2 = slim.convolution2d(dis1,256,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
        "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
        "        reuse=reuse,scope='d_conv2', weights_initializer=initializer)\n",
        "    \n",
        "        dis3 = slim.convolution2d(dis2,128,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
        "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
        "        reuse=reuse,scope='d_conv3',weights_initializer=initializer)\n",
        "      \n",
        "        dis4 = slim.convolution2d(dis3,64,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
        "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
        "        reuse=reuse,scope='d_conv4',weights_initializer=initializer)\n",
        "      \n",
        "        dis5 = slim.convolution2d(dis4,64,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
        "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
        "        reuse=reuse,scope='d_conv5',weights_initializer=initializer)\n",
        "      \n",
        "        dis6 = slim.convolution2d(dis5,128,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
        "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
        "        reuse=reuse,scope='d_conv6',weights_initializer=initializer)\n",
        "      \n",
        "        dis7 = slim.convolution2d(dis6,512,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
        "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
        "        reuse=reuse,scope='d_conv7',weights_initializer=initializer)\n",
        "        \n",
        "        dis8 = slim.convolution2d(dis7,256,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
        "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
        "        reuse=reuse,scope='d_conv8',weights_initializer=initializer)\n",
        "        \n",
        "        dis9 = slim.convolution2d(dis8,512,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
        "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
        "        reuse=reuse,scope='d_conv9',weights_initializer=initializer)\n",
        "        \n",
        "        dis10 = slim.convolution2d(dis9,256,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
        "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
        "        reuse=reuse,scope='d_conv10',weights_initializer=initializer)\n",
        "        \n",
        "        dis11 = slim.convolution2d(dis10,512,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
        "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
        "        reuse=reuse,scope='d_conv11',weights_initializer=initializer)\n",
        "        \n",
        "        dis12 = slim.convolution2d(dis11,256,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
        "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
        "        reuse=reuse,scope='d_conv12',weights_initializer=initializer)\n",
        "        \n",
        "        dis13 = slim.convolution2d(dis12,512,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
        "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
        "        reuse=reuse,scope='d_conv13',weights_initializer=initializer)\n",
        "        \n",
        "        dis14 = slim.convolution2d(dis13,1024,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
        "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
        "        reuse=reuse,scope='d_conv14',weights_initializer=initializer)\n",
        "        \n",
        "        dis15 = slim.convolution2d(dis14,512,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
        "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
        "        reuse=reuse,scope='d_conv15',weights_initializer=initializer)\n",
        "    \n",
        "        d_out = slim.fully_connected(slim.flatten(dis14),1,activation_fn=tf.nn.sigmoid,\\\n",
        "        reuse=reuse,scope='d_out', weights_initializer=initializer)\n",
        "    \n",
        "    return d_out\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "BTUXl4vf0l38"
      },
      "cell_type": "markdown",
      "source": [
        "###Putting the two together"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HbNJDAP40oSO",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
        "\n",
        "z_in = tf.placeholder(shape=[None,NOISE_SIZE],dtype=tf.float32) #Random vector\n",
        "real_in = tf.placeholder(shape=[None,512,512,1],dtype=tf.float32) #Real images\n",
        "\n",
        "Gz = generator(z_in) \n",
        "Dx = discriminator(real_in) \n",
        "Dg = discriminator(Gz) \n",
        "\n",
        "\n",
        "d_loss = -tf.reduce_mean(tf.log(Dx) + tf.log(1.-Dg)) #This optimizes the discriminator.\n",
        "g_loss = -tf.reduce_mean(tf.log(Dg)) #This optimizes the generator.\n",
        "\n",
        "tvars = tf.trainable_variables()\n",
        "\n",
        "G_vars = [var for var in tvars if 'generator' in var.name]\n",
        "D_vars = [var for var in tvars if 'discriminator' in var.name]\n",
        "\n",
        "#The below code is responsible for applying gradient descent to update the GAN.\n",
        "trainerD = tf.train.AdamOptimizer(learning_rate=0.0002).minimize(d_loss, var_list=D_vars)\n",
        "trainerG = tf.train.AdamOptimizer(learning_rate=0.0002).minimize(g_loss, var_list=G_vars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MBoagxC31o0G"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4UG8sRfa1q6Y",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 2 #Size of image batch to apply at each iteration.\n",
        "num_batches = len(dataloader)\n",
        "iterations = 1000 #Total number of iterations to use.\n",
        "\n",
        "# Start interactive session\n",
        "session = tf.InteractiveSession()\n",
        "# Init Variables\n",
        "tf.global_variables_initializer().run()\n",
        "# Init Logger\n",
        "logger = Logger(model_name='DCGAN2', data_name='MNIST')\n",
        "for i in range(iterations):\n",
        "    for n_batch, (batch,_) in enumerate(dataloader):\n",
        "            \n",
        "        # 1. Train Discriminator\n",
        "        X_batch = batch.permute(0, 2, 3, 1).numpy()\n",
        "        feed_dict = {real_in: X_batch, z_in: np.random.normal(size=(BATCH_SIZE, NOISE_SIZE))}\n",
        "        _, d_error, d_pred_real, d_pred_fake = session.run(\n",
        "            [trainerD, d_loss, Dx, Dg], feed_dict=feed_dict\n",
        "        )\n",
        "\n",
        "        # 2. Train Generator\n",
        "        #Run generator twice\n",
        "        for j in range(2):\n",
        "            feed_dict = {z_in: np.random.normal(size=(BATCH_SIZE, NOISE_SIZE))}\n",
        "            _, g_error = session.run([trainerG, g_loss], feed_dict=feed_dict)\n",
        "        \n",
        "        \n",
        "        if n_batch % BATCH_SIZE == 0:\n",
        "            display.clear_output(True)\n",
        "            # Generate images from test noise\n",
        "            z2 = np.random.uniform(-1.0,1.0,size=[16,NOISE_SIZE]).astype(np.float32) #Generate another z batch\n",
        "            test_images = session.run(\n",
        "                Gz, feed_dict={z_in: z2}\n",
        "            )\n",
        "            # Log Images\n",
        "            logger.log_images(test_images, 16, i, n_batch, num_batches, format='NHWC');\n",
        "            # Log Status\n",
        "            logger.display_status(\n",
        "                i, iterations, n_batch, num_batches,\n",
        "                d_error, g_error, d_pred_real, d_pred_fake\n",
        "            )\n",
        "            #Log Variables\n",
        "            logger.log(d_error, g_error, i, n_batch, num_batches)\n",
        "            #logger.save_models(Gz, Dg, i)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
